{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multistep Consistency Models — Darcy Flow\n",
    "\n",
    "Implementation of [\"Multistep Consistency Models\"](https://arxiv.org/abs/2403.06807) (Heek, Hoogeboom, Salimans 2024).\n",
    "\n",
    "**Task:** Unconditional generation of Darcy Flow PDE solution fields u(x,y).\n",
    "\n",
    "**Dataset:** PDEBench 2D Darcy Flow (1024 samples, 1x128x128).\n",
    "\n",
    "**Two-phase pipeline:**\n",
    "1. Train a VP Diffusion teacher model\n",
    "2. Distill into a fast Consistency Model (2-8 step generation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: GPU Check & Drive Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "assert torch.cuda.is_available(), \"Enable GPU: Runtime > Change runtime type > GPU\"\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Setup — Install Deps & Load Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies (PyTorch is pre-installed in Colab)\n!pip install -q omegaconf einops tensorboard h5py\n\n# Clone repo (pull latest on re-run)\nimport os\nREPO_DIR = '/content/GenModeling'\nif os.path.exists(REPO_DIR):\n    !git -C {REPO_DIR} pull\nelse:\n    !git clone https://github.com/MehdiMHeydari/GenModeling.git {REPO_DIR}\n\nimport sys\nif REPO_DIR not in sys.path:\n    sys.path.insert(0, REPO_DIR)\nprint(\"Repo loaded and path configured.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.models.networks.unet.unet import UNetModelWrapper as UNetModel\n",
    "from src.models.vp_diffusion import VPDiffusionModel\n",
    "from src.models.consistency_models import MultistepConsistencyModel\n",
    "from src.training.trainer import Trainer\n",
    "from src.training.objectives import VPDiffusionLoss, MultistepCDLoss\n",
    "from src.inference.samplers import MultistepCMSampler\n",
    "from src.utils.dataset import VF_FM\n",
    "from src.utils.logger import configure, log, logkvs, dumpkvs\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "print(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Darcy Flow: 1 channel, 128x128 spatial\n",
    "DATA_SHAPE = (1, 128, 128)\n",
    "\n",
    "# Dataset path on Google Drive\n",
    "DATA_PATH = \"/content/drive/MyDrive/2D_DarcyFlow_beta1.0_Train.hdf5\"\n",
    "\n",
    "# UNet config\n",
    "UNET_CFG = dict(\n",
    "    dim=list(DATA_SHAPE),\n",
    "    channel_mult=\"1, 2, 4, 4\",\n",
    "    num_channels=64,\n",
    "    num_res_blocks=2,\n",
    "    num_head_channels=32,\n",
    "    attention_resolutions=\"32\",\n",
    "    dropout=0.0,\n",
    "    use_new_attention_order=True,\n",
    "    use_scale_shift_norm=True,\n",
    "    class_cond=False,\n",
    "    num_classes=None,\n",
    ")\n",
    "\n",
    "# Training config — Teacher (VP Diffusion)\n",
    "TEACHER_EPOCHS = 200\n",
    "TEACHER_BATCH_SIZE = 16\n",
    "TEACHER_LR = 1e-4\n",
    "TEACHER_SAVE_DIR = \"/content/drive/MyDrive/cd_darcy/teacher\"\n",
    "TEACHER_SCHEDULE_S = 0.008\n",
    "\n",
    "# Training config — Student (Consistency Distillation)\n",
    "CD_EPOCHS = 100\n",
    "CD_BATCH_SIZE = 16\n",
    "CD_LR = 1e-4\n",
    "CD_STUDENT_STEPS = 2\n",
    "CD_EMA_RATE = 0.9999\n",
    "CD_X_VAR_FRAC = 0.75\n",
    "CD_HUBER_EPS = 1e-4\n",
    "CD_SAVE_DIR = \"/content/drive/MyDrive/cd_darcy/student\"\n",
    "\n",
    "# Sampling config\n",
    "SAMPLE_BATCH_SIZE = 8\n",
    "NUM_SAMPLES = 24\n",
    "SAMPLE_SAVE_PATH = \"/content/drive/MyDrive/cd_darcy/samples.pt\"\n",
    "\n",
    "os.makedirs(TEACHER_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(CD_SAVE_DIR, exist_ok=True)\n",
    "print(\"Configuration set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Load & Preprocess Darcy Flow Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD DARCY FLOW DATASET\n",
    "# ============================================================\n",
    "# File: 2D_DarcyFlow_beta1.0_Train.hdf5 (PDEBench)\n",
    "#   \"nu\":     (N, 128, 128)    — input permeability (NOT USED)\n",
    "#   \"tensor\": (N, 1, 128, 128) — output solution u(x,y) (USED)\n",
    "# ============================================================\n",
    "\n",
    "with h5py.File(DATA_PATH, 'r') as f:\n",
    "    print(\"HDF5 keys:\", list(f.keys()))\n",
    "    outputs = np.array(f['tensor']).astype(np.float32)\n",
    "    print(f\"Raw tensor shape: {outputs.shape}\")\n",
    "\n",
    "# Handle shape: ensure (N, 1, 128, 128)\n",
    "if outputs.ndim == 3:\n",
    "    outputs = outputs[:, np.newaxis, :, :]\n",
    "    print(f\"Added channel dim -> {outputs.shape}\")\n",
    "\n",
    "assert outputs.shape[1:] == (1, 128, 128), f\"Unexpected shape: {outputs.shape}\"\n",
    "print(f\"Raw data range: [{outputs.min():.4f}, {outputs.max():.4f}]\")\n",
    "\n",
    "# --- Min-max normalize to [-1, 1] ---\n",
    "data_min = float(outputs.min())\n",
    "data_max = float(outputs.max())\n",
    "outputs_norm = 2.0 * (outputs - data_min) / (data_max - data_min) - 1.0\n",
    "print(f\"Normalized range: [{outputs_norm.min():.4f}, {outputs_norm.max():.4f}]\")\n",
    "\n",
    "# Save normalization stats for denormalization later\n",
    "np.save(os.path.join(TEACHER_SAVE_DIR, \"data_min.npy\"), np.array(data_min))\n",
    "np.save(os.path.join(TEACHER_SAVE_DIR, \"data_max.npy\"), np.array(data_max))\n",
    "\n",
    "# --- Train / Val / Test split ---\n",
    "train_data = outputs_norm[:800]\n",
    "val_data   = outputs_norm[800:1000]\n",
    "test_data  = outputs_norm[1000:]\n",
    "\n",
    "print(f\"Train: {train_data.shape[0]} samples\")\n",
    "print(f\"Val:   {val_data.shape[0]} samples\")\n",
    "print(f\"Test:  {test_data.shape[0]} samples\")\n",
    "\n",
    "# --- Build dataset and dataloader ---\n",
    "dataset = VF_FM(train_data, all_vel=True)\n",
    "train_loader = DataLoader(\n",
    "    dataset, batch_size=TEACHER_BATCH_SIZE,\n",
    "    shuffle=True, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nSample shape: {dataset.shape}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Build UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet():\n",
    "    return UNetModel(**UNET_CFG)\n",
    "\n",
    "# Sanity check\n",
    "net = build_unet()\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "print(f\"UNet parameters: {total_params:,}\")\n",
    "\n",
    "# Quick forward pass test\n",
    "with th.no_grad():\n",
    "    C, H, W = DATA_SHAPE\n",
    "    x_test = th.randn(2, C, H, W)\n",
    "    t_test = th.tensor([0.3, 0.7])\n",
    "    out = net(t=t_test, x=x_test)\n",
    "    print(f\"Forward pass: input {x_test.shape} -> output {out.shape}\")\n",
    "    assert out.shape == x_test.shape\n",
    "del net, x_test, t_test, out\n",
    "print(\"UNet OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Train VP Diffusion Teacher (Phase A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 1: Train VP Diffusion Teacher\n",
    "# ============================================================\n",
    "\n",
    "teacher_network = build_unet()\n",
    "teacher_model = VPDiffusionModel(\n",
    "    network=teacher_network, schedule_s=TEACHER_SCHEDULE_S\n",
    ")\n",
    "\n",
    "optimizer = Adam(teacher_model.network.parameters(), lr=TEACHER_LR)\n",
    "objective = VPDiffusionLoss(class_conditional=False)\n",
    "\n",
    "logpath_teacher = os.path.join(TEACHER_SAVE_DIR, \"logs\")\n",
    "os.makedirs(logpath_teacher, exist_ok=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=teacher_model,\n",
    "    objective=objective,\n",
    "    dataloader=train_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=None,\n",
    "    logger_dict={\n",
    "        \"dir\": logpath_teacher,\n",
    "        \"format_strs\": [\"log\", \"tensorboard\"],\n",
    "        \"log_print_freq\": 5,\n",
    "    },\n",
    "    checkpointing_dict={\n",
    "        \"restart\": False,\n",
    "        \"restart_epoch\": None,\n",
    "        \"save_path\": TEACHER_SAVE_DIR,\n",
    "        \"save_epoch_int\": 25,\n",
    "        \"log_batch_int\": 10,\n",
    "    },\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "trainer.train(num_epochs=TEACHER_EPOCHS)\n",
    "print(f\"Teacher training complete. Checkpoints in {TEACHER_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Train CD Student (Phase B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 2: Consistency Distillation\n",
    "# ============================================================\n",
    "\n",
    "# Pick teacher checkpoint (last saved epoch)\n",
    "TEACHER_CKPT = os.path.join(\n",
    "    TEACHER_SAVE_DIR,\n",
    "    f\"checkpoint_{(TEACHER_EPOCHS - 1) // 25 * 25}.pt\"\n",
    ")\n",
    "assert os.path.exists(TEACHER_CKPT), f\"No teacher checkpoint at {TEACHER_CKPT}\"\n",
    "\n",
    "# --- Load frozen teacher ---\n",
    "teacher_net = build_unet()\n",
    "teacher = VPDiffusionModel(\n",
    "    network=teacher_net, schedule_s=TEACHER_SCHEDULE_S, infer=True\n",
    ")\n",
    "state = th.load(TEACHER_CKPT, map_location='cpu', weights_only=True)\n",
    "teacher.network.load_state_dict(state['model_state_dict'])\n",
    "teacher.to(DEVICE)\n",
    "teacher.eval()\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad_(False)\n",
    "print(f\"Loaded teacher from {TEACHER_CKPT}\")\n",
    "\n",
    "# --- Build student (initialize from teacher weights) ---\n",
    "student_net = build_unet()\n",
    "student_net.load_state_dict(teacher_net.state_dict())\n",
    "\n",
    "student_model = MultistepConsistencyModel(\n",
    "    network=student_net,\n",
    "    student_steps=CD_STUDENT_STEPS,\n",
    "    schedule_s=TEACHER_SCHEDULE_S,\n",
    "    ema_rate=CD_EMA_RATE,\n",
    ")\n",
    "student_model.to(DEVICE)\n",
    "\n",
    "# --- CD Loss ---\n",
    "cd_loss = MultistepCDLoss(\n",
    "    class_conditional=False,\n",
    "    teacher_model=teacher,\n",
    "    student_steps=CD_STUDENT_STEPS,\n",
    "    x_var_frac=CD_X_VAR_FRAC,\n",
    "    huber_epsilon=CD_HUBER_EPS,\n",
    "    schedule_s=TEACHER_SCHEDULE_S,\n",
    ")\n",
    "\n",
    "optimizer_cd = Adam(student_model.network.parameters(), lr=CD_LR)\n",
    "\n",
    "# Rebuild loader with CD batch size\n",
    "cd_loader = DataLoader(\n",
    "    dataset, batch_size=CD_BATCH_SIZE,\n",
    "    shuffle=True, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "# --- Training loop ---\n",
    "logpath_cd = os.path.join(CD_SAVE_DIR, \"logs\")\n",
    "os.makedirs(logpath_cd, exist_ok=True)\n",
    "configure(dir=logpath_cd, format_strs=[\"log\", \"tensorboard\"])\n",
    "\n",
    "for epoch in range(CD_EPOCHS):\n",
    "    student_model.network.train()\n",
    "    total_loss = 0.0\n",
    "    for i, batch in enumerate(cd_loader):\n",
    "        loss = cd_loss(student_model, batch, device=DEVICE)\n",
    "        optimizer_cd.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_cd.step()\n",
    "        student_model.update_ema()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(cd_loader)\n",
    "    logkvs({\"epoch\": epoch, \"cd_loss\": avg_loss})\n",
    "    dumpkvs()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}: loss={avg_loss:.6f}, \"\n",
    "              f\"teacher_steps={cd_loss._teacher_step_schedule()}\")\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        ckpt = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': student_model.network.state_dict(),\n",
    "            'ema_state_dict': student_model.ema_network.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_cd.state_dict(),\n",
    "        }\n",
    "        th.save(ckpt, f\"{CD_SAVE_DIR}/checkpoint_{epoch}.pt\")\n",
    "\n",
    "print(\"CD training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Sample from Trained CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 3: Generate Samples\n",
    "# ============================================================\n",
    "\n",
    "# Pick the last saved CD checkpoint\n",
    "CD_CKPT = os.path.join(\n",
    "    CD_SAVE_DIR,\n",
    "    f\"checkpoint_{(CD_EPOCHS - 1) // 10 * 10}.pt\"\n",
    ")\n",
    "\n",
    "# Build and load model\n",
    "sample_net = build_unet()\n",
    "cm = MultistepConsistencyModel(\n",
    "    network=sample_net,\n",
    "    student_steps=CD_STUDENT_STEPS,\n",
    "    schedule_s=TEACHER_SCHEDULE_S,\n",
    "    infer=True,\n",
    ")\n",
    "state = th.load(CD_CKPT, map_location='cpu', weights_only=True)\n",
    "cm.network.load_state_dict(state['model_state_dict'])\n",
    "if 'ema_state_dict' in state:\n",
    "    cm.ema_network.load_state_dict(state['ema_state_dict'])\n",
    "cm.to(DEVICE)\n",
    "cm.eval()\n",
    "\n",
    "sampler = MultistepCMSampler(cm)\n",
    "\n",
    "# Generate\n",
    "C, H, W = DATA_SHAPE\n",
    "rounds = (NUM_SAMPLES + SAMPLE_BATCH_SIZE - 1) // SAMPLE_BATCH_SIZE\n",
    "all_samples = []\n",
    "\n",
    "print(f\"Generating {NUM_SAMPLES} samples with {CD_STUDENT_STEPS} steps...\")\n",
    "with th.no_grad():\n",
    "    for i in range(rounds):\n",
    "        n = min(SAMPLE_BATCH_SIZE, NUM_SAMPLES - i * SAMPLE_BATCH_SIZE)\n",
    "        z = th.randn(n, C, H, W, device=DEVICE)\n",
    "        samples = sampler.sample(z)\n",
    "        all_samples.append(samples.cpu())\n",
    "        print(f\"  Batch {i+1}/{rounds} done\")\n",
    "\n",
    "all_samples = th.cat(all_samples, dim=0)[:NUM_SAMPLES]\n",
    "th.save(all_samples, SAMPLE_SAVE_PATH)\n",
    "print(f\"Saved {NUM_SAMPLES} samples to {SAMPLE_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Visualize — Generated vs Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load normalization stats\n",
    "data_min = float(np.load(os.path.join(TEACHER_SAVE_DIR, \"data_min.npy\")))\n",
    "data_max = float(np.load(os.path.join(TEACHER_SAVE_DIR, \"data_max.npy\")))\n",
    "\n",
    "def denormalize(x_norm):\n",
    "    \"\"\"Map from [-1, 1] back to physical units.\"\"\"\n",
    "    return (x_norm + 1.0) / 2.0 * (data_max - data_min) + data_min\n",
    "\n",
    "# Denormalize generated samples\n",
    "gen_np = denormalize(all_samples.numpy())  # (N, 1, 128, 128)\n",
    "\n",
    "# Denormalize test data for comparison\n",
    "test_denorm = denormalize(test_data)  # (24, 1, 128, 128)\n",
    "\n",
    "# --- Plot: top row = generated, bottom row = real ---\n",
    "n_show = 4\n",
    "fig, axes = plt.subplots(2, n_show, figsize=(4 * n_show, 8))\n",
    "\n",
    "vmin = min(gen_np[:n_show].min(), test_denorm[:n_show].min())\n",
    "vmax = max(gen_np[:n_show].max(), test_denorm[:n_show].max())\n",
    "\n",
    "for i in range(n_show):\n",
    "    im = axes[0, i].imshow(\n",
    "        gen_np[i, 0], cmap='viridis', vmin=vmin, vmax=vmax\n",
    "    )\n",
    "    axes[0, i].set_title(f\"Generated {i+1}\")\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    axes[1, i].imshow(\n",
    "        test_denorm[i, 0], cmap='viridis', vmin=vmin, vmax=vmax\n",
    "    )\n",
    "    axes[1, i].set_title(f\"Real {i+1}\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "fig.colorbar(im, ax=axes, shrink=0.6, label='u(x,y)')\n",
    "plt.suptitle(\n",
    "    f\"Darcy Flow: CM Samples ({CD_STUDENT_STEPS} steps) vs Real\",\n",
    "    fontsize=14\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/darcy_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- Grid of all generated samples ---\n",
    "n_grid = min(NUM_SAMPLES, 16)\n",
    "rows = (n_grid + 3) // 4\n",
    "fig, axes = plt.subplots(rows, 4, figsize=(16, 4 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(len(axes)):\n",
    "    if i < n_grid:\n",
    "        axes[i].imshow(gen_np[i, 0], cmap='viridis')\n",
    "        axes[i].set_title(f\"Sample {i+1}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\n",
    "    f\"All Generated Darcy Flow Fields ({CD_STUDENT_STEPS}-step CM)\",\n",
    "    fontsize=14\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/darcy_grid.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated range: [{gen_np.min():.4f}, {gen_np.max():.4f}]\")\n",
    "print(f\"Real range:      [{test_denorm.min():.4f}, {test_denorm.max():.4f}]\")\n",
    "print(\"Visualization complete.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}