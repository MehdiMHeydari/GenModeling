{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multistep Consistency Models — Darcy Flow\n",
    "\n",
    "Implementation of [\"Multistep Consistency Models\"](https://arxiv.org/abs/2403.06807) (Heek, Hoogeboom, Salimans 2024).\n",
    "\n",
    "**Task:** Unconditional generation of Darcy Flow PDE solution fields u(x,y).\n",
    "\n",
    "**Dataset:** PDEBench 2D Darcy Flow (1024 samples, 1x128x128).\n",
    "\n",
    "**Two-phase pipeline:**\n",
    "1. Train a VP Diffusion teacher model\n",
    "2. Distill into a fast Consistency Model (2-8 step generation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: GPU Check & Drive Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nassert torch.cuda.is_available(), \"Enable GPU: Runtime > Change runtime type > GPU\"\nprint(f\"GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\nfrom google.colab import drive\ndrive.mount('/content/drive')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Setup — Install Deps & Load Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies (PyTorch is pre-installed in Colab)\n!pip install -q omegaconf einops tensorboard h5py\n\n# Clone repo (pull latest on re-run)\nimport os\nREPO_DIR = '/content/GenModeling'\nif os.path.exists(REPO_DIR):\n    !git -C {REPO_DIR} pull\nelse:\n    !git clone https://github.com/MehdiMHeydari/GenModeling.git {REPO_DIR}\n\nimport sys\nif REPO_DIR not in sys.path:\n    sys.path.insert(0, REPO_DIR)\nprint(\"Repo loaded and path configured.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.models.networks.unet.unet import UNetModelWrapper as UNetModel\n",
    "from src.models.vp_diffusion import VPDiffusionModel\n",
    "from src.models.consistency_models import MultistepConsistencyModel\n",
    "from src.training.trainer import Trainer\n",
    "from src.training.objectives import VPDiffusionLoss, MultistepCDLoss\n",
    "from src.inference.samplers import MultistepCMSampler\n",
    "from src.utils.dataset import VF_FM\n",
    "from src.utils.logger import configure, log, logkvs, dumpkvs\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "print(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Darcy Flow: 1 channel, 128x128 spatial\n",
    "DATA_SHAPE = (1, 128, 128)\n",
    "\n",
    "# Dataset path on Google Drive\n",
    "DATA_PATH = \"/content/drive/MyDrive/2D_DarcyFlow_beta1.0_Train.hdf5\"\n",
    "\n",
    "# UNet config\n",
    "UNET_CFG = dict(\n",
    "    dim=list(DATA_SHAPE),\n",
    "    channel_mult=\"1, 2, 4, 4\",\n",
    "    num_channels=64,\n",
    "    num_res_blocks=2,\n",
    "    num_head_channels=32,\n",
    "    attention_resolutions=\"32\",\n",
    "    dropout=0.0,\n",
    "    use_new_attention_order=True,\n",
    "    use_scale_shift_norm=True,\n",
    "    class_cond=False,\n",
    "    num_classes=None,\n",
    ")\n",
    "\n",
    "# Training config — Teacher (VP Diffusion)\n",
    "TEACHER_EPOCHS = 200\n",
    "TEACHER_BATCH_SIZE = 16\n",
    "TEACHER_LR = 1e-4\n",
    "TEACHER_SAVE_DIR = \"/content/drive/MyDrive/cd_darcy/teacher\"\n",
    "TEACHER_SCHEDULE_S = 0.008\n",
    "\n",
    "# Training config — Student (Consistency Distillation)\n",
    "CD_EPOCHS = 100\n",
    "CD_BATCH_SIZE = 16\n",
    "CD_LR = 1e-4\n",
    "CD_STUDENT_STEPS = 2\n",
    "CD_EMA_RATE = 0.9999\n",
    "CD_X_VAR_FRAC = 0.75\n",
    "CD_HUBER_EPS = 1e-4\n",
    "CD_SAVE_DIR = \"/content/drive/MyDrive/cd_darcy/student\"\n",
    "\n",
    "# Sampling config\n",
    "SAMPLE_BATCH_SIZE = 8\n",
    "NUM_SAMPLES = 24\n",
    "SAMPLE_SAVE_PATH = \"/content/drive/MyDrive/cd_darcy/samples.pt\"\n",
    "\n",
    "os.makedirs(TEACHER_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(CD_SAVE_DIR, exist_ok=True)\n",
    "print(\"Configuration set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Load & Preprocess Darcy Flow Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD DARCY FLOW DATASET\n",
    "# ============================================================\n",
    "# File: 2D_DarcyFlow_beta1.0_Train.hdf5 (PDEBench)\n",
    "#   \"nu\":     (N, 128, 128)    — input permeability (NOT USED)\n",
    "#   \"tensor\": (N, 1, 128, 128) — output solution u(x,y) (USED)\n",
    "# ============================================================\n",
    "\n",
    "with h5py.File(DATA_PATH, 'r') as f:\n",
    "    print(\"HDF5 keys:\", list(f.keys()))\n",
    "    outputs = np.array(f['tensor']).astype(np.float32)\n",
    "    print(f\"Raw tensor shape: {outputs.shape}\")\n",
    "\n",
    "# Handle shape: ensure (N, 1, 128, 128)\n",
    "if outputs.ndim == 3:\n",
    "    outputs = outputs[:, np.newaxis, :, :]\n",
    "    print(f\"Added channel dim -> {outputs.shape}\")\n",
    "\n",
    "assert outputs.shape[1:] == (1, 128, 128), f\"Unexpected shape: {outputs.shape}\"\n",
    "print(f\"Raw data range: [{outputs.min():.4f}, {outputs.max():.4f}]\")\n",
    "\n",
    "# --- Min-max normalize to [-1, 1] ---\n",
    "data_min = float(outputs.min())\n",
    "data_max = float(outputs.max())\n",
    "outputs_norm = 2.0 * (outputs - data_min) / (data_max - data_min) - 1.0\n",
    "print(f\"Normalized range: [{outputs_norm.min():.4f}, {outputs_norm.max():.4f}]\")\n",
    "\n",
    "# Save normalization stats for denormalization later\n",
    "np.save(os.path.join(TEACHER_SAVE_DIR, \"data_min.npy\"), np.array(data_min))\n",
    "np.save(os.path.join(TEACHER_SAVE_DIR, \"data_max.npy\"), np.array(data_max))\n",
    "\n",
    "# --- Train / Val / Test split ---\n",
    "train_data = outputs_norm[:800]\n",
    "val_data   = outputs_norm[800:1000]\n",
    "test_data  = outputs_norm[1000:]\n",
    "\n",
    "print(f\"Train: {train_data.shape[0]} samples\")\n",
    "print(f\"Val:   {val_data.shape[0]} samples\")\n",
    "print(f\"Test:  {test_data.shape[0]} samples\")\n",
    "\n",
    "# --- Build dataset and dataloader ---\n",
    "dataset = VF_FM(train_data, all_vel=True)\n",
    "train_loader = DataLoader(\n",
    "    dataset, batch_size=TEACHER_BATCH_SIZE,\n",
    "    shuffle=True, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nSample shape: {dataset.shape}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Build UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet():\n",
    "    return UNetModel(**UNET_CFG)\n",
    "\n",
    "# Sanity check\n",
    "net = build_unet()\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "print(f\"UNet parameters: {total_params:,}\")\n",
    "\n",
    "# Quick forward pass test\n",
    "with th.no_grad():\n",
    "    C, H, W = DATA_SHAPE\n",
    "    x_test = th.randn(2, C, H, W)\n",
    "    t_test = th.tensor([0.3, 0.7])\n",
    "    out = net(t=t_test, x=x_test)\n",
    "    print(f\"Forward pass: input {x_test.shape} -> output {out.shape}\")\n",
    "    assert out.shape == x_test.shape\n",
    "del net, x_test, t_test, out\n",
    "print(\"UNet OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Train VP Diffusion Teacher (Phase A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# STAGE 1: Train VP Diffusion Teacher\n# ============================================================\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, clear_output\n\nteacher_network = build_unet()\nteacher_model = VPDiffusionModel(\n    network=teacher_network, schedule_s=TEACHER_SCHEDULE_S\n)\nteacher_model.to(DEVICE)\n\noptimizer = Adam(teacher_model.network.parameters(), lr=TEACHER_LR)\nobjective = VPDiffusionLoss(class_conditional=False)\n\n# --- Training loop with live loss plot ---\nloss_history = []\nbest_loss = float('inf')\nfig, ax = plt.subplots(1, 1, figsize=(10, 4))\nplot_handle = display(fig, display_id=True)\n\nepoch_bar = tqdm(range(TEACHER_EPOCHS), desc=\"Teacher Training\", unit=\"epoch\")\n\nfor epoch in epoch_bar:\n    teacher_model.network.train()\n    total_loss = 0.0\n\n    for batch in train_loader:\n        loss = objective(teacher_model, batch, device=DEVICE)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n    loss_history.append(avg_loss)\n    best_loss = min(best_loss, avg_loss)\n    epoch_bar.set_postfix(loss=f\"{avg_loss:.6f}\", best=f\"{best_loss:.6f}\")\n\n    # Update live plot every 5 epochs\n    if epoch % 5 == 0 or epoch == TEACHER_EPOCHS - 1:\n        ax.clear()\n        ax.plot(loss_history, color='tab:blue', linewidth=1.5)\n        ax.set_xlabel('Epoch')\n        ax.set_ylabel('Loss')\n        ax.set_title(f'Teacher Training — Epoch {epoch}, Loss: {avg_loss:.6f}, Best: {best_loss:.6f}')\n        ax.set_yscale('log')\n        ax.grid(True, alpha=0.3)\n        plot_handle.update(fig)\n\n    # Save checkpoint\n    if epoch % 25 == 0:\n        state = {\n            'epoch': epoch,\n            'model_state_dict': teacher_model.network.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'sched_state_dict': None,\n        }\n        th.save(state, f\"{TEACHER_SAVE_DIR}/checkpoint_{epoch}.pt\")\n        tqdm.write(f\"  Saved checkpoint at epoch {epoch}\")\n\nplt.close(fig)\nprint(f\"\\nTeacher training complete. Best loss: {best_loss:.6f}\")\nprint(f\"Checkpoints in {TEACHER_SAVE_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Train CD Student (Phase B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# STAGE 2: Consistency Distillation\n# ============================================================\n\n# Pick teacher checkpoint (last saved epoch)\nTEACHER_CKPT = os.path.join(\n    TEACHER_SAVE_DIR,\n    f\"checkpoint_{(TEACHER_EPOCHS - 1) // 25 * 25}.pt\"\n)\nassert os.path.exists(TEACHER_CKPT), f\"No teacher checkpoint at {TEACHER_CKPT}\"\n\n# --- Load frozen teacher ---\nteacher_net = build_unet()\nteacher = VPDiffusionModel(\n    network=teacher_net, schedule_s=TEACHER_SCHEDULE_S, infer=True\n)\nstate = th.load(TEACHER_CKPT, map_location='cpu', weights_only=True)\nteacher.network.load_state_dict(state['model_state_dict'])\nteacher.to(DEVICE)\nteacher.eval()\nfor p in teacher.parameters():\n    p.requires_grad_(False)\nprint(f\"Loaded teacher from {TEACHER_CKPT}\")\n\n# --- Build student (initialize from teacher weights) ---\nstudent_net = build_unet()\nstudent_net.load_state_dict(teacher_net.state_dict())\n\nstudent_model = MultistepConsistencyModel(\n    network=student_net,\n    student_steps=CD_STUDENT_STEPS,\n    schedule_s=TEACHER_SCHEDULE_S,\n    ema_rate=CD_EMA_RATE,\n)\nstudent_model.to(DEVICE)\n\n# --- CD Loss ---\ncd_loss = MultistepCDLoss(\n    class_conditional=False,\n    teacher_model=teacher,\n    student_steps=CD_STUDENT_STEPS,\n    x_var_frac=CD_X_VAR_FRAC,\n    huber_epsilon=CD_HUBER_EPS,\n    schedule_s=TEACHER_SCHEDULE_S,\n)\n\noptimizer_cd = Adam(student_model.network.parameters(), lr=CD_LR)\n\n# Rebuild loader with CD batch size\ncd_loader = DataLoader(\n    dataset, batch_size=CD_BATCH_SIZE,\n    shuffle=True, num_workers=2, pin_memory=True\n)\n\n# --- Training loop with live loss plot ---\ncd_loss_history = []\nbest_loss = float('inf')\nfig, ax = plt.subplots(1, 1, figsize=(10, 4))\nplot_handle = display(fig, display_id=True)\n\nepoch_bar = tqdm(range(CD_EPOCHS), desc=\"CD Training\", unit=\"epoch\")\n\nfor epoch in epoch_bar:\n    student_model.network.train()\n    total_loss = 0.0\n\n    for batch in cd_loader:\n        loss = cd_loss(student_model, batch, device=DEVICE)\n        optimizer_cd.zero_grad()\n        loss.backward()\n        optimizer_cd.step()\n        student_model.update_ema()\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(cd_loader)\n    cd_loss_history.append(avg_loss)\n    best_loss = min(best_loss, avg_loss)\n    n_teacher = cd_loss._teacher_step_schedule()\n    epoch_bar.set_postfix(loss=f\"{avg_loss:.6f}\", best=f\"{best_loss:.6f}\", N_t=n_teacher)\n\n    # Update live plot every 5 epochs\n    if epoch % 5 == 0 or epoch == CD_EPOCHS - 1:\n        ax.clear()\n        ax.plot(cd_loss_history, color='tab:orange', linewidth=1.5)\n        ax.set_xlabel('Epoch')\n        ax.set_ylabel('CD Loss')\n        ax.set_title(f'CD Training — Epoch {epoch}, Loss: {avg_loss:.6f}, Best: {best_loss:.6f}, N_teacher: {n_teacher}')\n        ax.set_yscale('log')\n        ax.grid(True, alpha=0.3)\n        plot_handle.update(fig)\n\n    # Save checkpoint\n    if epoch % 10 == 0:\n        ckpt = {\n            'epoch': epoch,\n            'model_state_dict': student_model.network.state_dict(),\n            'ema_state_dict': student_model.ema_network.state_dict(),\n            'optimizer_state_dict': optimizer_cd.state_dict(),\n        }\n        th.save(ckpt, f\"{CD_SAVE_DIR}/checkpoint_{epoch}.pt\")\n        tqdm.write(f\"  Saved checkpoint at epoch {epoch}\")\n\nplt.close(fig)\nprint(f\"\\nCD training complete. Best loss: {best_loss:.6f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Sample from Trained CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 3: Generate Samples\n",
    "# ============================================================\n",
    "\n",
    "# Pick the last saved CD checkpoint\n",
    "CD_CKPT = os.path.join(\n",
    "    CD_SAVE_DIR,\n",
    "    f\"checkpoint_{(CD_EPOCHS - 1) // 10 * 10}.pt\"\n",
    ")\n",
    "\n",
    "# Build and load model\n",
    "sample_net = build_unet()\n",
    "cm = MultistepConsistencyModel(\n",
    "    network=sample_net,\n",
    "    student_steps=CD_STUDENT_STEPS,\n",
    "    schedule_s=TEACHER_SCHEDULE_S,\n",
    "    infer=True,\n",
    ")\n",
    "state = th.load(CD_CKPT, map_location='cpu', weights_only=True)\n",
    "cm.network.load_state_dict(state['model_state_dict'])\n",
    "if 'ema_state_dict' in state:\n",
    "    cm.ema_network.load_state_dict(state['ema_state_dict'])\n",
    "cm.to(DEVICE)\n",
    "cm.eval()\n",
    "\n",
    "sampler = MultistepCMSampler(cm)\n",
    "\n",
    "# Generate\n",
    "C, H, W = DATA_SHAPE\n",
    "rounds = (NUM_SAMPLES + SAMPLE_BATCH_SIZE - 1) // SAMPLE_BATCH_SIZE\n",
    "all_samples = []\n",
    "\n",
    "print(f\"Generating {NUM_SAMPLES} samples with {CD_STUDENT_STEPS} steps...\")\n",
    "with th.no_grad():\n",
    "    for i in range(rounds):\n",
    "        n = min(SAMPLE_BATCH_SIZE, NUM_SAMPLES - i * SAMPLE_BATCH_SIZE)\n",
    "        z = th.randn(n, C, H, W, device=DEVICE)\n",
    "        samples = sampler.sample(z)\n",
    "        all_samples.append(samples.cpu())\n",
    "        print(f\"  Batch {i+1}/{rounds} done\")\n",
    "\n",
    "all_samples = th.cat(all_samples, dim=0)[:NUM_SAMPLES]\n",
    "th.save(all_samples, SAMPLE_SAVE_PATH)\n",
    "print(f\"Saved {NUM_SAMPLES} samples to {SAMPLE_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Visualize — Generated vs Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load normalization stats\n",
    "data_min = float(np.load(os.path.join(TEACHER_SAVE_DIR, \"data_min.npy\")))\n",
    "data_max = float(np.load(os.path.join(TEACHER_SAVE_DIR, \"data_max.npy\")))\n",
    "\n",
    "def denormalize(x_norm):\n",
    "    \"\"\"Map from [-1, 1] back to physical units.\"\"\"\n",
    "    return (x_norm + 1.0) / 2.0 * (data_max - data_min) + data_min\n",
    "\n",
    "# Denormalize generated samples\n",
    "gen_np = denormalize(all_samples.numpy())  # (N, 1, 128, 128)\n",
    "\n",
    "# Denormalize test data for comparison\n",
    "test_denorm = denormalize(test_data)  # (24, 1, 128, 128)\n",
    "\n",
    "# --- Plot: top row = generated, bottom row = real ---\n",
    "n_show = 4\n",
    "fig, axes = plt.subplots(2, n_show, figsize=(4 * n_show, 8))\n",
    "\n",
    "vmin = min(gen_np[:n_show].min(), test_denorm[:n_show].min())\n",
    "vmax = max(gen_np[:n_show].max(), test_denorm[:n_show].max())\n",
    "\n",
    "for i in range(n_show):\n",
    "    im = axes[0, i].imshow(\n",
    "        gen_np[i, 0], cmap='viridis', vmin=vmin, vmax=vmax\n",
    "    )\n",
    "    axes[0, i].set_title(f\"Generated {i+1}\")\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    axes[1, i].imshow(\n",
    "        test_denorm[i, 0], cmap='viridis', vmin=vmin, vmax=vmax\n",
    "    )\n",
    "    axes[1, i].set_title(f\"Real {i+1}\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "fig.colorbar(im, ax=axes, shrink=0.6, label='u(x,y)')\n",
    "plt.suptitle(\n",
    "    f\"Darcy Flow: CM Samples ({CD_STUDENT_STEPS} steps) vs Real\",\n",
    "    fontsize=14\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/darcy_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- Grid of all generated samples ---\n",
    "n_grid = min(NUM_SAMPLES, 16)\n",
    "rows = (n_grid + 3) // 4\n",
    "fig, axes = plt.subplots(rows, 4, figsize=(16, 4 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(len(axes)):\n",
    "    if i < n_grid:\n",
    "        axes[i].imshow(gen_np[i, 0], cmap='viridis')\n",
    "        axes[i].set_title(f\"Sample {i+1}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\n",
    "    f\"All Generated Darcy Flow Fields ({CD_STUDENT_STEPS}-step CM)\",\n",
    "    fontsize=14\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/darcy_grid.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated range: [{gen_np.min():.4f}, {gen_np.max():.4f}]\")\n",
    "print(f\"Real range:      [{test_denorm.min():.4f}, {test_denorm.max():.4f}]\")\n",
    "print(\"Visualization complete.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}